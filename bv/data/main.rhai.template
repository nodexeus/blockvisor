/// You can define some constants to use in the code
const NETS = #{
    {{variant_key}}: #{
        url: "https://testnet-api.helium.wtf/v1/",
        net_type: "test",
        seeds: "test seed"
    },
};

/// Variant-specific store keys for per-client snapshots
/// R1.2: Each service can have its own store key per variant
const VARIANTS = #{
    "reth-mainnet-archive": #{
        net: "mainnet",
        reth_store_key: "ethereum-reth-mainnet-archive-v1",
        lighthouse_store_key: "ethereum-lighthouse-mainnet-archive-v1",
    },
    "reth-mainnet-full": #{
        net: "mainnet",
        reth_store_key: "ethereum-reth-mainnet-full-v1",
        lighthouse_store_key: "ethereum-lighthouse-mainnet-archive-v1", // intentionally left as archive because the lighthouse dataset is the same between full and archive
    },
    "reth-testnet-archive": #{
        net: "testnet",
        reth_store_key: "ethereum-reth-testnet-archive-v1",
        lighthouse_store_key: "ethereum-lighthouse-testnet-archive-v1",
    },
};

const RETH_STORE_KEY = VARIANTS[node_env().node_variant].reth_store_key;
const LIGHTHOUSE_STORE_KEY = VARIANTS[node_env().node_variant].lighthouse_store_key;

const API_HOST = "http://localhost:4467/";
const RETH_DIR = node_env().protocol_data_path + "/reth";
const LIGHTHOUSE_DIR = node_env().protocol_data_path + "/lighthouse";
const NET = global::NETS[node_env().node_variant];

fn plugin_config() {
  #{
    /// [optional] List of configuration files to be rendered from template with provided params.
    /// See [Tera Docs](https://keats.github.io/tera/docs/#templates) for details on templating syntax.
    config_files: [
        #{
            /// It assumes that file pointed by `template` argument exists.
            template: "/var/lib/babel/templates/config.template",
            /// File pointed by `destination` path will be overwritten if exists.
            destination: "/etc/service.config",
            /// Map object serializable to JSON.
            params: #{
                node_name: node_env().node_name
            },
        },
    ],
    /// [optional] List of auxiliary services that doesn't need init steps and doesn't use protocol data.
    aux_services: [
       #{
           /// Unique service job name.
           name: "aux_service_a",
           /// Sh script body. It shall be blocking (foreground) process.
           run_sh: `monitoring_agent`,
           /// [optional] Job restart config.
           /// If not set default to:
           /// #{
           ///    backoff_timeout_ms: 60000,
           ///    backoff_base_ms: 1000,
           /// }
           restart_config: #{
               /// if job stay alive given amount of time (in milliseconds) backoff is reset
               backoff_timeout_ms: 60000,
               /// base time (in milliseconds) for backoff,
               /// multiplied by consecutive power of 2 each time
               backoff_base_ms: 1000,
               /// [optional] maximum number of retries, or there is no such limit if not set
               max_retries: 13,
           },
           /// [optional] Job shutdown timeout - how long it may take to gracefully shutdown the job.
           /// After given time job won't be killed, but babel will rise the error.
           /// If not set default to 60s.
           shutdown_timeout_secs: 120,
           /// [optional] POSIX signal that will be sent to child processes on job shutdown.
           /// See [man7](https://man7.org/linux/man-pages/man7/signal.7.html) for possible values.
           /// If not set default to `SIGTERM`.
           shutdown_signal: "SIGINT",
           /// [optional] Run job as a different user.
           run_as: "some_user",
           /// [optional] Capacity of log buffer (in megabytes).
           /// If not set default to 128.
           log_buffer_capacity_mb: 256,
           /// [optional] Prepend timestamp to each log, or not.
           /// If not set default to false.
           log_timestamp: true,
       },
    ],
    /// Node init actions.
    init: #{
        /// List of sh commands to be executed first.
        commands: [
            `mkdir -p /opt/netdata/var/cache/netdata`,
        ],
        /// List of sh jobs (long running tasks), to be started then.
        jobs: [
            #{
                /// Unique job name.
                name: "init_job",
                /// Sh script body. It shall be blocking (foreground) process.
                run_sh: `openssl rand -hex 32 > ${global::RETH_DIR}/jwt.txt`,
                /// [optional] InitJob restart policy.
                /// If not set default to "never".
                restart: "never",
                /// [optional] Job shutdown timeout - how long it may take to gracefully shutdown the job.
                /// After given time job won't be killed, but babel will rise the error.
                /// If not set default to 60s.
                shutdown_timeout_secs: 120,
                /// [optional] POSIX signal that will be sent to child processes on job shutdown.
                /// See [man7](https://man7.org/linux/man-pages/man7/signal.7.html) for possible values.
                /// If not set default to `SIGTERM`.
                shutdown_signal: "SIGINT",
                /// [optional] List of job names that this job needs to be finished before start.
                needs: ["other_init_job_name"],
                /// [optional] Run job as a different user.
                run_as: "some_user",
                /// [optional] Capacity of log buffer (in megabytes).
                /// If not set default to 128.
                log_buffer_capacity_mb: 32,
                /// [optional] Prepend timestamp to each log, or not.
                /// If not set default to false.
                log_timestamp: true,
                /// [optional] Indicate if job should run only once.
                /// One-time jobs never run again, even after node upgrade.
                /// If not set default to false.
                one_time: true,
                /// [optional] Flag indicating if job uses protocol data.
                /// Job that uses protocol data, automatically create lock that prevents
                /// re-initialization of the data after job is started.
                /// If not set default to false.
                use_protocol_data: false,
            }
        ]
    },
    /// List of services to be started once init jobs and download is finished.
    services: [
        #{
            name: "reth",
            /// data_dir: "reth",                                 // R1.1: Optional, defaults to service name
            store_key: global::RETH_STORE_KEY,                   // R1.2: Individual store key
            run_sh: `/usr/bin/reth node --datadir ${global::RETH_DIR} --chain mainnet --http --http-addr 0.0.0.0 "$@"`,
            use_protocol_data: true,
            /// [optional] Job restart config.
            /// If not set default to:
            /// #{
            ///    backoff_timeout_ms: 60000,
            ///    backoff_base_ms: 1000,
            /// }
            restart_config: #{
                /// if job stay alive given amount of time (in milliseconds) backoff is reset
                backoff_timeout_ms: 60000,
                /// base time (in milliseconds) for backoff,
                /// multiplied by consecutive power of 2 each time
                backoff_base_ms: 1000,
                /// [optional] maximum number of retries, or there is no such limit if not set
                max_retries: 13,
            },
            /// [optional] Job shutdown timeout - how long it may take to gracefully shutdown the job.
            /// After given time job won't be killed, but babel will rise the error.
            /// If not set default to 60s.
            shutdown_timeout_secs: 120,
            /// [optional] POSIX signal that will be sent to child processes on job shutdown.
            /// See [man7](https://man7.org/linux/man-pages/man7/signal.7.html) for possible values.
            /// If not set default to `SIGTERM`.
            shutdown_signal: "SIGINT",
            /// [optional] Run job as a different user.
            run_as: "some_user",
            /// [optional] Capacity of log buffer (in megabytes).
            /// If not set default to 128.
            log_buffer_capacity_mb: 256,
            /// [optional] Prepend timestamp to each log, or not.
            /// If not set default to false.
            log_timestamp: true,
        },
        #{
            name: "lighthouse",
            /// data_dir defaults to "lighthouse" since name is "lighthouse"
            store_key: global::LIGHTHOUSE_STORE_KEY,
            run_sh: `/usr/bin/lighthouse beacon_node --datadir ${global::LIGHTHOUSE_DIR} --network mainnet --execution-endpoint http://127.0.0.1:8551 "$@"`,
            use_protocol_data: true,
        }
    ],
    /// [optional] Download configuration.
    /// Built-in download will be started once all init jobs are finished.
    download: #{
        /// [optional] Maximum number of parallel opened connections.
        /// If not set default to 3.
        max_connections: 5,
        /// [optional] Maximum number of parallel workers.
        /// If not set default to 8.
        max_runners: 8,
        /// [optional] Job restart config.
        /// If not set default to:
        /// #{
        ///    backoff_timeout_ms: 600000,
        ///    backoff_base_ms: 500,
        ///    max_retries: 10,
        /// }
        restart_config: #{
            backoff_timeout_ms: 60000,
            backoff_base_ms: 1000,
            max_retries: 5,
        },
    },
    /// [optional] Alternative download if archive for standard one is not available.
    alternative_download: #{
        /// Sh script body. It shall be blocking (foreground) process.
        run_sh: `/usr/bin/wget -q -O - some_url`,
        /// [optional] InitJob restart config.
        /// If not set default to "never".
        restart_config: #{
            backoff_timeout_ms: 60000,
            backoff_base_ms: 10000,
            max_retries: 3,
        },
        /// [optional] Run job as a different user.
        run_as: "some_user",
        /// [optional] Capacity of log buffer (in megabytes).
        /// If not set default to 128.
        log_buffer_capacity_mb: 64,
        /// [optional] Prepend timestamp to each log, or not.
        /// If not set default to false.
        log_timestamp: true,
    },
    /// [optional] Post-download sh jobs, to be started after download.
    post_download: [
        #{
            name: "post_download_job",
            run_sh: `echo restoreDB`,
            use_protocol_data: true,
        }
    ],
    /// [optional] Alternative protocol data initialization.
    /// It is fallback job, run only if neither regular archive nor `alternative_download` is available.
    /// NOTE: `post_download` is not run after `cold_init`.
    cold_init: #{
        /// Sh script body. It shall be blocking (foreground) process.
        run_sh: `/usr/bin/wget -q -O - some_url`,
        /// [optional] InitJob restart config.
        /// If not set default to "never".
        restart_config: #{
            backoff_timeout_ms: 60000,
            backoff_base_ms: 10000,
            max_retries: 3,
        },
        /// [optional] Run job as a different user.
        run_as: "some_user",
        /// [optional] Capacity of log buffer (in megabytes).
        /// If not set default to 128.
        log_buffer_capacity_mb: 64,
        /// [optional] Prepend timestamp to each log, or not.
        /// If not set default to false.
        log_timestamp: true,
        /// [optional] Indicate if job should run only once.
        /// One-time jobs never run again, even after node upgrade.
        /// If not set default to false.
        one_time: true,
    },
    /// [optional] Pre-upload action.
    pre_upload: #{
        /// list of sh commands to be executed before upload
        commands: [
            `echo uploading`,
        ],
        /// list of sh jobs (long running tasks), to be started before upload
        jobs: [
            #{
                name: "pre_upload_job",
                run_sh: `echo dumpDB`,
            }
        ]
    },
    /// [optional] Upload configuration.
    /// Built-in upload can be manually triggered with `bv node run upload`.
    upload: #{
        /// [optional] List of exclude patterns. Files in `node_env().protocol_data_path` directory that match any of pattern,
        /// won't be taken into account.
        exclude: [
            "**/something_to_ignore*",
            ".gitignore",
            "some_subdir/*.bak",
        ],
        /// [optional] Compression to be used on chunks.
        /// If not set default to `ZSTD: 3`.
        compression: #{
            ZSTD: 5, /// compression level
        },
        /// [optional] Maximum number of parallel opened connections.
        /// If not set default to 3.
        max_connections: 4,
        /// [optional] Maximum number of parallel workers.
        /// If not set default to 8.
        max_runners: 12,
        /// [optional] Number of chunks that protocol data should be split into.
        /// Recommended chunk size is about 500MB. Estimated by BV based on data size, if not provided.
        /// If not set calculated automatically by BV.
        number_of_chunks: 700,
        /// [optional] Seconds after which presigned urls in generated `UploadManifest` may expire.
        /// If not set calculated automatically by BV.
        url_expires_secs: 240000,
        /// [optional] Version number for uploaded data. Auto-assigned if not provided.
        /// If not set calculated automatically by Blockvisor API.
        data_version: 3,
        /// [optional] Job restart config.
        /// If not set default to:
        /// #{
        ///    backoff_timeout_ms: 600000,
        ///    backoff_base_ms: 500,
        ///    max_retries: 10,
        /// }
        restart_config: #{
            backoff_timeout_ms: 60000,
            backoff_base_ms: 1000,
            max_retries: 5,
        },
    },
    /// [optional] Post-upload sh jobs, to be started after upload.
    post_upload: [
        #{
            name: "post_upload_job",
            run_sh: `echo cleanup_after_upload`,
        }
    ],
    /// [optional] List of tasks to be scheduled on init.
    scheduled: [
        #{
            /// Unique name of the task.
            name: "some_task",
            /// Cron schedule expression.
            schedule: "* * * * * * *",
            /// Function name to be executed according to schedule.
            function: "fn_name",
            /// [optional] Parameter to ba passed to function.
            param: "param_value",
        }
    ],
  }
}

/// Returns protocol application status.
/// SHALL be implemented
fn protocol_status() {
    /// "uploading", "downloading", "starting" statuses are automatically handled by BV if BABEL_CONFIG const is defined
    let result = run_jrpc(#{
            host: global::API_HOST,
            method: "health.health",
            headers: [["content-type", "application/json"]]
        }).expect(200);

    if result.healthy != "false" || result.result.checks.bootstrapped.error == "subnets not bootstrapped" {
        #{state: "broadcasting", health: "healthy"}
    } else {
        #{state: "delinquent", health: "healthy"}
    }
}
